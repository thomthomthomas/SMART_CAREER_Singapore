{
    "main_role": "AI Data Engineer",
    "skills_breakdown": [
        {
            "skill": "Data Mining",
            "videos": [
                {
                    "video_id": "7rs0i-9nOjo",
                    "title": "What is Data Mining?",
                    "channel": "IBM Technology",
                    "view_count": 170790,
                    "duration": "6m 53s",
                    "transcript": "If you've ever been panning for gold, you'll know that it takes a lot of time and effort to find even a small nugget. It's estimated that to extract enough go to make a single gold ring, you'd need to sort through around twenty six tons of rock and other stuff. That's a lot to sift through. The same is true when mining data, except the gold is replaced with insights and the panning is replaced with algorithms. So let's talk about it. Data mining. So data mining is the process of extracting valuable information from large datasets, and it's used in a variety of industries, from marketing through to health care. And it can help businesses to make more informed decisions. Now, fundamentally, data mining is about processing data and identifying patterns and trends in that information. And when we think about the evolution of things like data warehouses, and when we think about things like just the sheer volume of data, big data. We can really start to see that these sort of data mining techniques have rapidly accelerated over the last couple of decades. We need to process so much of this data and turn it into useful knowledge. One of the main advantages of data mining is that it can help you to make predictions about future trends. By analyzing past data, you can build up a picture of how things might develop in the future. Data mining can also help you to identify relationships between different pieces of data that you might not have been able to see before. So, for example, you might see that there is a correlation between the amount of time somebody spends on your website and the likelihood of them making a purchase. Now we can think of the data mining process consisting of four basic steps. So step one is setting objectives. And this is where data scientists and business stakeholders work together to define a business problem that data mining will be applied to. Now, with the problem defined with the scope defined, we move onto step two, which is data preparation. This identifies which set of data it will help answer these pertinent questions to the business that we set in step one. Now, there's more here than just identifying the data. We also need to clean it, removing any noise, such as duplicates, missing values, and outliers. Then we move on to stage three, which is applying the data. And applying it specifically through data mining algorithms. We're looking here for interesting data relationships and applying deep learning techniques -- and we'll look deeper into step three in just a second. Then finally, step four is evaluating results. So this is really interpreting results that are valid, novel, useful and understandable. So let's talk about some of those data mining techniques that make up stage three here. Data mining works by using various algorithms and techniques to turn large volumes of data into useful information. And while there are many ways to do this, here are some of the most common - and let's start with kind of the most straightforward, which is association. Now, association is rule-based, and it's a method for finding relationships between variables in a given dataset. You make a simple correlation between two or more items, often with the same type, to identify patterns. So, for example, when tracking people's buying habits, you might identify that a customer always buys cream and then they tend to buy strawberries. And therefore, you could suggest that the next time they buy strawberries, they might also want to purchase cream. You can use another technique called classification as well. And classification does, is this builds up the idea of the type of customer or the type of item or the type of object by describing multiple attributes to identify a particular class. So, for example, you could easily classify cars into different types like sedan, 4x4, convertible, and you could do that by identifying different attributes like the number of seats or the shape of the car. Then, given a new car, you can apply it into a particular class by comparing the attributes with our known definition. Another useful technique is clustering. Now, clustering enables you to group individual pieces of data together to form a structure. Correlating the data instances with other examples so you can see where the similarities and the ranges agree. There are a number of deep learning techniques utilizing artificial neural networks as well that we can use to form things such as predictions. By analyzing past events or past instances, you can make a prediction about an event. If the input data is labeled, regression can be applied to predict the likelihood of a particular assignment. If the dataset isn't labeled, the individual data points and the training set are compared with one another to discover underlying similarities- clustering them based upon those shared characteristics. You’ll often see things like decision trees and K Nearest Neighbor, or KNN algorithms, used here. One of the most important things to remember is that data mining techniques are not a one-size-fits-all solution, with different techniques being more or less effective depending upon your data- your business questions and what you're trying to achieve. It's often a case of trial and error to identify which method will work best for you. So data mining... it combines business stakeholders and data scientists into this whole process shown here. And when done right, you can find [clears throat] golden insights that can be transformational for a business. If you have any questions, please drop us a line below, and if you want to see more videos like this in the future, please like and subscribe. Thanks for watching."
                },
                {
                    "video_id": "hTjo-QVWcK0",
                    "title": "What Does a Data Engineer ACTUALLY Do?",
                    "channel": "Learn with Lukas",
                    "view_count": 128659,
                    "duration": "5m 2s",
                    "transcript": "The data engineer is one of the highest paying data roles with relatively low\ncompetition and a great future. But what does the data engineer\nactually do? The simple answer is that they designed,\nbuild and maintain the infrastructure for collecting,\nstoring and analyzing data, ensuring it's accessible,\nreliable and optimized for performance. But this is a very broad explanation. So let's get into the details of what\nindeed engineer would do day to day, the responsibilities you may have,\nthe exact skills you need, how much data engineers make and finally, I'll compare\nthe data engineer to other data roles, because that is going to give you\na complete understanding of this role. When you understand how a data engineer\ninteracts with other data roles in a team. Let's get started. The engineer might start today\nby monitoring and checking the health and operating of different data\npipelines and databases, because it's going to be up to them\nto ensure that things run smoothly. The data fuels other business functions,\nsuch as the work of a data analyst or a data scientist. So it's important that everything works. Next, they may optimize the performance\nof the databases and data processing tasks because dealing with large data\nsets take a lot of time and power. So we want to make things efficient. Now, when it comes to more specific tasks, the engineer is responsible for developing\nand maintaining ETL processes. ETL stands for Extract Transform Load,\nand it's basically about getting the right data from various sources\ninto the right places where we need it. For example, our own database. Next, a data engineer might do some data\ncleansing because through all of this we need to make sure that the data we have is of high quality\nand that it doesn't contain any problems. Of course, you also want the data in the right format\nduring the day or during the week. A data engineer will also collaborate\nmultiple times with other team members and stakeholders like data scientists,\nanalysts and other clients. For example, it can be requesting access\nto the right data or just making sure\nwe're meeting all the expectations. Of course,\na data engineer will do more things. They may create documentation, security\nmeasures and explore ways to upgrade the existing systems for increased\nefficiency and better capabilities. But these are some of the core\nresponsibilities of a data engineer. Today, let's talk about the salary\nyou can expect as a data engineer, and then we'll talk\nmore about the in-depth skills and compared the data engineer\nto the other data roles in the US. When you start off as a data engineer,\nyou're going to make roughly 100,000. It's going to depend a lot on your state,\nthe company and so on. But we can see that\naccording to Glassdoor, the range is about 83 to 130000. For a data engineer. For a senior data engineer,\nthe average salary is around 136,000 per year, of course, with a large range,\ndepending on different factors. And for you lead engineer, we're looking\nat around an average of 153,000 per year. Even if you're not located in the US,\nI think it should still give you a general understanding of the salary level\nand the data engineer. It paid it very well. Now let's quickly cover the skills\nto become a data engineer. This is according to indeed\npopular employment websites. First we have the coding skills. He did engineer will have solid programing skills with Python\nbeing really important for data engineers. They can also use a variety\nof other programing languages depending on the situation\nto accomplish tasks. So you do need to have strong programing\nfundamentals. Next, you'll want to be familiar with\ndatabase systems and database management. To do this, you will need to know sequel,\nwhich is a key skill, knowledge of diverse database solutions\nand an understanding of data. Warehousing is also essential. Now to work with big data. As a data engineer, you will need\nto understand the relevant tools. For example, Apache Spark Cloud is also\nvery important in today's landscape as a data engineer, where Microsoft Azure,\na US and Google Cloud platform and so on are really important skills\nand these are the most popular as well. Now I do want to emphasize\nthat you don't need to learn them all, but rather look at what companies\nyou want to work at and what they use. If you don't know this,\nthat's completely fine. And I think that's normal. US and Microsoft,\nthese are some of the most popular. It is also necessary to have a strong\nunderstanding of data analysis in itself. But in general, landing a data engineering role is not going to be the easiest thing because your work is really critical\nand can cause a lot of damage. It's done incorrectly. So you could run the data\npipeline or whatever, and therefore you often need\nsome data experience before you become a data engineer. I could mention a lot of more things, but I feel like there's no need\nin repeating myself. The data engineer is going to do\na lot of different things. I could talk for days,\nbut these are really the key skills. It's still unclear. Here is a job\nlisting from a major tech company where you can see\nwhat they actually desire yourself. Now we're going to compare it\nto some other data roles. And when comparing the data engineer\nto other data roles, it is clear that your focus more\non the architecture itself of the data and you're kind of building the foundation\nfor the data in the company. This data may then be used by other team\nmembers for analysis, for machine learning or whatever. You're not really going to work in the spotlight,\nbut your work is really important. Now, the data engineer is\njust one of many amazing data rules. And to learn more about other data\nrules as well, check out this video next. And have an amazing week, guys."
                }
            ],
            "subskills": [
                "Data Cleaning and Preprocessing: Handling missing values, outlier detection, data transformation (normalization, standardization), feature scaling, data reduction techniques.",
                "Data Exploration and Visualization: Descriptive statistics, data visualization libraries (Matplotlib, Seaborn, Tableau), exploratory data analysis (EDA) techniques, identifying patterns and trends.",
                "Data Mining Algorithms: Classification (Logistic Regression, Decision Trees, Support Vector Machines, Naive Bayes), Regression (Linear Regression, Polynomial Regression), Clustering (K-means, hierarchical clustering), Association Rule Mining (Apriori algorithm).",
                "Database Management Systems (DBMS): SQL queries (SELECT, JOIN, WHERE, GROUP BY, HAVING), NoSQL databases (MongoDB, Cassandra), database design principles.",
                "Model Evaluation and Selection: Accuracy, precision, recall, F1-score, AUC-ROC curve, confusion matrix, cross-validation techniques, model tuning (hyperparameter optimization).",
                "Big Data Technologies: Hadoop, Spark, MapReduce, cloud-based data warehousing (AWS S3, Azure Data Lake).",
                "Predictive Modeling: Building predictive models using machine learning algorithms, model deployment and monitoring.",
                "Data Storytelling and Communication: Effectively communicating findings through visualizations and reports, presenting insights to stakeholders."
            ],
            "key_takeaways": [
                "Data mining is an iterative process requiring careful planning, data preparation, and model evaluation.",
                "The quality of the insights derived directly depends on the quality of the data used.",
                "Effective data visualization is crucial for communicating insights to both technical and non-technical audiences.",
                "Different algorithms are suited to different types of data and analytical goals. Selecting the appropriate algorithm is key.",
                "Data mining can reveal hidden patterns and trends that lead to improved decision-making and business outcomes.",
                "Ethical considerations regarding data privacy and bias mitigation are paramount."
            ],
            "important_info": [
                "A strong foundation in statistics and mathematics is crucial for understanding and applying data mining techniques.",
                "Proficiency in programming languages such as Python or R is essential for implementing data mining algorithms and analyzing data.",
                "Understanding of data structures and algorithms is beneficial for optimizing data processing and model performance.",
                "Staying updated with the latest advancements in data mining techniques and tools is essential for remaining competitive in the field.",
                "Adherence to data governance and ethical guidelines is non-negotiable to ensure responsible data use."
            ],
            "summary": "Data mining is a highly valuable skill in today's data-driven world, enabling professionals to extract actionable insights from massive datasets.  Its applications span diverse industries, from marketing and finance to healthcare and scientific research.  Data mining professionals leverage statistical techniques, machine learning algorithms, and database management skills to identify patterns, trends, and anomalies, ultimately informing strategic decisions and driving business innovation.  Mastery of this skill significantly enhances career prospects, offering lucrative opportunities in data science, business analytics, and related fields.  Success in this domain requires not only technical expertise but also strong communication abilities to effectively convey complex data insights to a wider audience."
        },
        {
            "skill": "Big Data Technologies",
            "videos": [
                {
                    "video_id": "hTjo-QVWcK0",
                    "title": "What Does a Data Engineer ACTUALLY Do?",
                    "channel": "Learn with Lukas",
                    "view_count": 128659,
                    "duration": "5m 2s",
                    "transcript": "The data engineer is one of the highest paying data roles with relatively low\ncompetition and a great future. But what does the data engineer\nactually do? The simple answer is that they designed,\nbuild and maintain the infrastructure for collecting,\nstoring and analyzing data, ensuring it's accessible,\nreliable and optimized for performance. But this is a very broad explanation. So let's get into the details of what\nindeed engineer would do day to day, the responsibilities you may have,\nthe exact skills you need, how much data engineers make and finally, I'll compare\nthe data engineer to other data roles, because that is going to give you\na complete understanding of this role. When you understand how a data engineer\ninteracts with other data roles in a team. Let's get started. The engineer might start today\nby monitoring and checking the health and operating of different data\npipelines and databases, because it's going to be up to them\nto ensure that things run smoothly. The data fuels other business functions,\nsuch as the work of a data analyst or a data scientist. So it's important that everything works. Next, they may optimize the performance\nof the databases and data processing tasks because dealing with large data\nsets take a lot of time and power. So we want to make things efficient. Now, when it comes to more specific tasks, the engineer is responsible for developing\nand maintaining ETL processes. ETL stands for Extract Transform Load,\nand it's basically about getting the right data from various sources\ninto the right places where we need it. For example, our own database. Next, a data engineer might do some data\ncleansing because through all of this we need to make sure that the data we have is of high quality\nand that it doesn't contain any problems. Of course, you also want the data in the right format\nduring the day or during the week. A data engineer will also collaborate\nmultiple times with other team members and stakeholders like data scientists,\nanalysts and other clients. For example, it can be requesting access\nto the right data or just making sure\nwe're meeting all the expectations. Of course,\na data engineer will do more things. They may create documentation, security\nmeasures and explore ways to upgrade the existing systems for increased\nefficiency and better capabilities. But these are some of the core\nresponsibilities of a data engineer. Today, let's talk about the salary\nyou can expect as a data engineer, and then we'll talk\nmore about the in-depth skills and compared the data engineer\nto the other data roles in the US. When you start off as a data engineer,\nyou're going to make roughly 100,000. It's going to depend a lot on your state,\nthe company and so on. But we can see that\naccording to Glassdoor, the range is about 83 to 130000. For a data engineer. For a senior data engineer,\nthe average salary is around 136,000 per year, of course, with a large range,\ndepending on different factors. And for you lead engineer, we're looking\nat around an average of 153,000 per year. Even if you're not located in the US,\nI think it should still give you a general understanding of the salary level\nand the data engineer. It paid it very well. Now let's quickly cover the skills\nto become a data engineer. This is according to indeed\npopular employment websites. First we have the coding skills. He did engineer will have solid programing skills with Python\nbeing really important for data engineers. They can also use a variety\nof other programing languages depending on the situation\nto accomplish tasks. So you do need to have strong programing\nfundamentals. Next, you'll want to be familiar with\ndatabase systems and database management. To do this, you will need to know sequel,\nwhich is a key skill, knowledge of diverse database solutions\nand an understanding of data. Warehousing is also essential. Now to work with big data. As a data engineer, you will need\nto understand the relevant tools. For example, Apache Spark Cloud is also\nvery important in today's landscape as a data engineer, where Microsoft Azure,\na US and Google Cloud platform and so on are really important skills\nand these are the most popular as well. Now I do want to emphasize\nthat you don't need to learn them all, but rather look at what companies\nyou want to work at and what they use. If you don't know this,\nthat's completely fine. And I think that's normal. US and Microsoft,\nthese are some of the most popular. It is also necessary to have a strong\nunderstanding of data analysis in itself. But in general, landing a data engineering role is not going to be the easiest thing because your work is really critical\nand can cause a lot of damage. It's done incorrectly. So you could run the data\npipeline or whatever, and therefore you often need\nsome data experience before you become a data engineer. I could mention a lot of more things, but I feel like there's no need\nin repeating myself. The data engineer is going to do\na lot of different things. I could talk for days,\nbut these are really the key skills. It's still unclear. Here is a job\nlisting from a major tech company where you can see\nwhat they actually desire yourself. Now we're going to compare it\nto some other data roles. And when comparing the data engineer\nto other data roles, it is clear that your focus more\non the architecture itself of the data and you're kind of building the foundation\nfor the data in the company. This data may then be used by other team\nmembers for analysis, for machine learning or whatever. You're not really going to work in the spotlight,\nbut your work is really important. Now, the data engineer is\njust one of many amazing data rules. And to learn more about other data\nrules as well, check out this video next. And have an amazing week, guys."
                },
                {
                    "video_id": "dJA7k58zlA8",
                    "title": "How I'd become a data analyst (if i had to start over) in 2025",
                    "channel": "Agatha",
                    "view_count": 882787,
                    "duration": "8m 57s",
                    "transcript": "here's how I'd become a data analyst if I had to start over as a data analyst now with over 10 years experience I have many Lessons Learned and would take a completely different path anyone yes you can be a data analyst all within 6 months of self-study here's how by failing to prepare you are preparing to fail says Benjamin Franklin and without a plan you will get lost in the data analyst Journey so here is a 6mon road map that I personally would use I'd split up the road map into three different parts the skills the projects and the job applications and put a date on the calendar 6 months from now and work backwards from it so in months one to three I will work on skills in month four I would work on projects and then month five and six is the job application process and 6 months is very reasonable if you have about 3 to 4 hours to study every day and everybody has different schedules and different commitments but if that's something that you can commit to this Roat would work for you I also have a video on how I was able to self-study for 4 hours every day and if you're looking for tips there I will link the video above months 1 through three is skills knowing what I know today I would really just focus on learning three tools and that is Excel SQL and Tapo I would learn it in that order and just focus on just those three for 3 months I'm not including python Cloud technology other bi tools because most entry-level data analyst roles will only require you to know those three skills that I listed and from my personal experience as well those were the only three tools I used for years so that is enough to get you started and you can worry about learning the other skills after you Le the job Excel is the first tool that I would learn because Excel is still the most popular bi tool used across Industries and it's highly likely that in your first role as a data analyst you'll be using a lot of excel you need to know how to clean analyze and present data using Excel and what level do you need about an intermediate level where you know key functions like vup pivot tables how to create visualizations and what I would do in my road map is break it down into smaller pieces so spend a week learning just the formulas and getting used to them and then spend the following week on visualizations and thinking through how you can visualize data using charts and graphs the best way to learn is by doing rather than moving on to SQL as soon as you finish Excel take the time to practice it what I would do is take my bank statement and try to do some analysis you can try answering questions like what does the trend of my spending look like and this would require taking all your spending and putting it on a line graph where you can see the trend of the increases and decreases of spending over time and then you can try bucketing your spending into categories like entertainment food and chart those categories over time to see how you're spending changes by category over time and this is the type of analysis that a data analyst would do next is seel seel is fundamental to be a data analyst if you have a technical assessment for your data analyst job it will 100% be in SQL data is stored in databases and SQL is a language that you use to talk to databases to get the information that you need SQL is a more powerful tool than Excel because it allows you to work with really large data set and also be able to combine them to easily create different analysis you only need to be an intermediate level SQL user note I didn't say Advanced and that includes functions like select wear Group by having joins and window functions some websites I personally use to learn SQL are data Camp Udi LinkedIn learning data Camp was extremely useful because it allowed me to interactively learn SQL handson without having to download anything as well as LinkedIn learning if you are in the US a lot of public libraries actually offer a free subscription so I was able to do a lot of my learning for free through Linkedin learning and lastly don't forget YouTube because there's a lot of really good free tutorials on YouTube that you should use as well next we move on to Tableau now any bi visualization tool can be used in place of Tableau such as powerbi or looker or quick ey but I personally would learn Tableau because it is the most commonly used tool that you'll see on job listings also from my experience when you start learning just one bi tool you know about 80% of others so it's really not as important which one you pick here so what should you know in Tau you should be about an intermediate user that knows how to connect to data add multiple data sources as well as creating visualizations with filters will get you pretty far there's also a free version of Tableau that you can download and start practicing and you can even take the same bank statement data that you had earlier and use that to create some of the same visualizations to get practice month four is projects this is a step that I wouldn't skip because without any data work experience I don't have anything to show recruiters that I know how to do data analysis so knowing that the purpose of these project projects is to use on my resume and interviews I would strategically use them to make myself stand out here are three tips for your projects tip one I'd create three to four projects that shows a combination of my skills so I wouldn't create a project in just Excel and a project in just SQL I would do a combination of Excel and SQL or SQL in Tableau and this shows the recruiter that I know which tools to use to solve which problems tip number two I create projects that have an analysis that solves a problem and tells a story data analysts always start their process first with the problem and it's no different for a project a common mistake is to start with the data set first with no plan and then work aimlessly without knowing what it is that you're going to do it's like when I'm looking at the bank statement data and without a problem statement I would just take it and create all sorts of fancy graphs that look really good but don't mean anything it's only when I have the problem statement of what does a trend of my spending look like that I actually have a direction and can sit down and think about well how would I best show this through a visualization and tell a story tip number three use free resources for data such as kaggle Reddit data.gov and even corser does have guided projects that you can pay for months 5 through 6 is the job application process I would prepare for interviews by updating my resume my LinkedIn and applying for data analyst jobs and I do have a video on how to create a data analyst resume with actionable tips which I'll leave up there for you to watch while applying for jobs I would start practicing technical questions pretty much every single data analyst job that I applied to has required a SQL technical assessment along with the general interview and I didn't know this and was completely unprepared but you know better so in month 5 give yourself enough time to take some technical test so that when it comes time for the actual interview you'll be ready I've personally used Le code hacker rank for SQL interview questions and they were actually pretty accurate to the ones that I gotten in real life so try and do the medium easy questions and if you can pass those then you'll be prepared with this R map I would start and then learn not learn and then start I hope this is the video that starts you on your data analyst journey and if you're interested in learning more about the soft skills a data analyst uses watch how to get ahead of 99% of data analyst and I will see you there"
                }
            ],
            "subskills": [
                "Data Warehousing: Designing data warehouses, dimensional modeling, ETL processes (Extract, Transform, Load), using tools like Snowflake, Amazon Redshift, Google BigQuery.",
                "Data Modeling: Conceptual, logical, and physical data modeling techniques; ER diagrams, schema design, normalization.",
                "Distributed Computing Frameworks: Hadoop, Spark, Flink; understanding parallel processing, distributed file systems (HDFS), and cluster management.",
                "NoSQL Databases: MongoDB, Cassandra, Redis; understanding document, key-value, and graph databases; choosing appropriate database technologies for various use cases.",
                "Cloud Computing Platforms for Big Data: AWS (EMR, S3, Redshift), Azure (HDInsight, Data Lake Storage), GCP (Dataproc, Cloud Storage); managing and deploying big data solutions in the cloud.",
                "Data Pipelines: Building and maintaining data pipelines using tools like Apache Kafka, Apache Airflow; implementing real-time and batch processing.",
                "Big Data Analytics: Statistical analysis, machine learning algorithms (regression, classification, clustering), and data visualization techniques for large datasets.",
                "Data Security and Governance: Implementing data security measures, access control, and compliance with data privacy regulations (GDPR, CCPA)."
            ],
            "key_takeaways": [
                "Big data technologies are crucial for extracting insights from massive, complex datasets, enabling data-driven decision-making across various industries.",
                "Choosing the right tools and technologies depends heavily on the specific data volume, velocity, variety, and veracity, as well as the desired analytical outcome.",
                "Effective data governance and security practices are paramount to protect sensitive information and maintain compliance.",
                "Data engineering roles require a combination of technical expertise, problem-solving skills, and the ability to work collaboratively within a team.",
                "Continuous learning is essential to stay updated with the rapidly evolving landscape of big data technologies and tools.",
                "Understanding both structured and unstructured data is crucial for effectively leveraging big data technologies."
            ],
            "important_info": [
                "Strong programming skills (e.g., Python, Java, Scala) are essential for many big data roles.",
                "Proficiency in SQL is crucial for interacting with relational databases and performing data analysis.",
                "Experience with cloud computing platforms is highly valuable in today's data-centric world.",
                "Understanding of data warehousing concepts and ETL processes is fundamental for building effective data pipelines.",
                "Familiarity with various data visualization tools (e.g., Tableau, Power BI) is beneficial for presenting data insights effectively."
            ],
            "summary": "Big data technologies are essential for organizations seeking to unlock the value hidden within massive datasets.  Professionals in this field need a robust understanding of data warehousing, distributed computing frameworks (like Hadoop and Spark), NoSQL databases, and cloud platforms.  They must master data modeling, ETL processes, and data analytics techniques, along with strong programming skills in languages such as Python or Java.  This skillset is highly valuable across diverse sectors, leading to high-demand careers in data engineering, data science, and data analysis.  Successful professionals exhibit a combination of technical expertise, problem-solving abilities, and a commitment to continuous learning, adapting to the ever-evolving tools and techniques within the big data landscape."
        },
        {
            "skill": "Cloud Computing",
            "videos": [
                {
                    "video_id": "IZLfO-94aNE",
                    "title": "How Generative AI Powered Data Pipelines Increase a Data Engineers Productivity",
                    "channel": "AWS Developers",
                    "view_count": 7974,
                    "duration": "7m 4s",
                    "transcript": "hi I'm Stephan Senior Solutions architect from AWS hi I'm Ed Thompson I'm CTO and co-founder of matian and this is my [Music] architecture hey okay thanks for bringing this architecture here today I see you using uh our generative AI service bedrock in this architecture and I would love to learn how you make the life of data and Engineers easier using generative AI in your platform a fantastic so um where all about matian making the data engineer as productive as possible and what our data integration tools does is allow them to build uh AI powered uh data pipelines um built on cloud data warehouses such as Amazon red shift so the way that works and the way that we've Incorporated uh the new AI uh platforms for AWS like Bedrock into the service is uh these data Engineers here they have a new service which is part of our product which is called the co-pilot which we run on Amazon eks hey what does the co-pilot do then so the co-pilot is two kind of main things first of all in the background it's collecting all of the information about how to build pipelines Loco pipelines in matian data productivity Cloud okay so it's doing that by collecting the product documentation by collecting the metadata from Amazon red shift that's the red shift from a customer right this is the customers metadata yeah so it's telling uh it's telling us what tables and uh columns they have SCH and so on and so and more detailed information about that and the product documentation is kind of the knowledge about material as a product exactly just like a human would read the manual to figure out how to use the product the LM needs to read the manual too okay I always try to think of llm as people uh it's a good way to kind of get a good result I often find and the final piece we have this uh component information service that is essentially saying this is exactly what component of the dag that the customer can use and a component of the dag might be something like a filter uh crossthe line calculations a window function something like that got it the DC is kind of the ETL job right BR down data flow yeah it's the transformation yeah absolutely and you're feeding all that into bedro using Titan yeah we're using Titan to do embeddings and then we actually store that data over here in Amazon RDS with its new PG Vector capability and that allows us to vectorize all of that information for the customer per customer got it got the second part of the story is um the the data engineer can then have a conversation with the tool um and that conversation is handled using the context from PG Vector in a uh rag or uh retrieval automated generation pipeline what would be a typical question that one of your data Engineers is asking that's good um so they typically be asking maybe something simple like how do I join these two tables together and we can use the inference and the Deemer and all the uh understanding that the large language model has to do that or it might even be a more complicated multi-step process like I have this Source I have this target table how do I map from one to the other effectively yeah that's usually very time consuming for the data Engineers right it is and it's kind of the bread but work of the data engineer but anything we can do to make that more productive is going to be great for the great for them and great for their company um so the conversation allows them uh the the large language model to modify uh what we call call our DPL or data productivity language this is a yaml based language that's really specifically designed uh for use with large language models okay that's a good information so design it that it's really U you know be able to be integrated with large language models such as Bedrock exactly and like I say it's got to be most human readable language that goes and that makes it work better with the with the large language model um and so what this does is it modifies uh the dag which we store in a in a service called um the uh working tree and that um is like a git service so everything going everything's version everything's going into git so nothing can be lost and then the final part of the process is those dags are then executed on on our workflow engine and the actual data transformation is pushed back down to the data warehouse such as red shift got and it's also presented in your user interface for the data engineer right exactly so they can see step-by-step process they can see the data being modified it's all visual uh and of course the nice thing about um doing rag in a visual tool as opposed to like a codebase C- pilot is it constrains the visual tool into exactly what it can do so you get much better results because you've got very key building blocks for the pipeline go it's a very important learning also that we share here with our customers cool then there say the second perspective can I as a data engineer also use those generative AI service you absolutely can so we have all of this fantastic technology we're using it to run our co-pilot but really what's most exciting to me is getting our customers to use to run their business um so what we want to be able to do and what we've been able to do in the data productivity cloud is essentially build components for all of the key pieces of AI technology so we've got components for loading into uh Vector databases components for doing embeddings and components for prompting and interacting with the large language model those are run through our workflow engine um and when you build a dag like that the really exciting thing is bringing together the customers data MH and the large language model that's the key thing um and when we do that with rag as well uh what you get is fantastic results for really some quite complex data operations so typically what's a customer doing they're doing things like um uh summarizing data uh they're doing sentiment analysis they're asking multiple questions over unstructured data turning unstructured data into structured data got it got it so what you do here is also remove the undifferentiated heavy lifting of tying those Services together right because you have the low code tooling available to your customers absolutely and it all starts with their data of course so it's their data in red shift that we're doing that transformation on got it well thank you for sharing this architecture with us what was your your journey with you know in integrating Bedrock into your platform and also making it available to your customers absolutely so um you know talking to our customers um The Journey with bedrock has been fantastic um customers really were quite Keen to understand what was going on behind the scenes with our co-pilot but also with the large language model they want to use bedrocks I think seen in the industry is very trusted uh way to run a large language model um and uh the kind of progress that we've seen in the platform and having access to really what are kind of Frontier top top tier models uh has been fantastic every time it gets upgraded our product gets better our customers pipeline gets better it's win-win awesome well thank you for sharing this architecture with us oh [Music]"
                },
                {
                    "video_id": "_a6us8kaq0g",
                    "title": "Cloud Computing Explained",
                    "channel": "PowerCert Animated Videos",
                    "view_count": 1100779,
                    "duration": "8m 37s",
                    "transcript": "Wat is wolkrekenaarkunde? So dit is die onderwerp van hierdie video. Nou het jy dalk gehoor van mense wat oor die wolk praat, soos wolkrekenaars of wolkberging, maar jy was waarskynlik nie seker presies wat dit was nie. Wel, die term wolkrekenaarkunde verwys na data en toepassings wat op die wolk gestoor en uitgevoer word eerder as om op jou plaaslike rekenaar of op enige toerusting wat jy besit gestoor en uitgevoer word . Dan word hierdie data en die toepassings wat op die wolk is, deur die internet verkry. So die werklading is nie meer op jou rekenaar of op enige toerusting wat jy besit nie, dit is op die wolk. So wat is die wolk? Nou om dit eenvoudig te stel, die wolk is net 'n groot gebou wat gevul is met rekenaars. Om spesifiek te wees, dit is 'n groot gebou vol bedieners en bedieners is net rekenaars wat dienste namens kliënte lewer. Nou is hierdie geboue baie groot, en dit moet goed wees, want as jy na binne kyk, is dit 'n reuse-datasentrum wat bedieners bevat so ver as wat die oog kan sien. En hierdie bedieners voer talle take uit, soos om toepassings te laat loop, data te stoor, dataverwerking, webhosting, ensovoorts. En hulle is ook almal saam genetwerk en hulle kan op die internet verkry word. So wat is die doel van 'n wolk? Wel, die maatskappye wat hierdie wolke besit, word wolkverskaffers genoem en hul doel is om hul rekenaars as 'n diens te verkoop. Nou is 'n diens net iets wat jy iemand betaal om vir jou te doen eerder as om die werk self te doen. As 'n persoon of 'n maatskappy dus 'n ander maatskappy wil huur om 'n gedeelte van of al hul rekenaarwerklading te doen, sal hulle dit aan 'n derde party uitkontrakteer. Met ander woorde, hulle sou wolkrekenaars gebruik So terug in die ou dae voor wolkrekenaars en as 'n voorbeeld sal ons e-pos gebruik. Dus by jou huis of kantoor as jy e-pos wil gebruik, sal jy jou eie fisiese e-posbediener hê. So jy sal ' n bediener, 'n bedryfstelsel en e-possagteware soos Microsoft Exchange hê. En dan na 'n paar konfigurasie, sal jy e-pos kan gebruik. Maar die probleem is, is dat as enigiets met die bediener verkeerd gaan , soos 'n hardewarefout of 'n sagtewareprobleem, of as die bedryfstelsel ineengestort het, dan sal jy verantwoordelik wees om die probleem reg te stel, om nie eers te praat van enige onderhoud wat nodig is nie. om die bediener aan die gang te hou. U het egter die opsie om al die rompslomp en instandhouding van u eie e-posbediener uit te skakel en 'n ander maatskappy te laat al u e-pos op hul bedieners in die wolk vir u aanbied, soos Gmail Hotmail en 'n klomp ander. Maar e-pos is net een voorbeeld van wolkrekenaars. Daar is ook ander dienste soos produktiwiteitsagteware, webbedieners, databasisse en selfs Youtube. So ja, jy as individu kan Youtube as 'n wolk gebruik. So as jy 'n videoskepper is en in plaas daarvan om jou eie videobediener en sagteware te bou en in stand te hou en die uiters hoë koste van internetbandwydte wat jy nodig het vir mense om jou video's te kyk vanaf jou bediener, kan jy dit omseil en jy kan net jou video's na Youtube oplaai en YouTube alles vir jou laat hanteer. Maar in plaas daarvan om YouTube direk te betaal soos 'n gewone wolkverskaffer, sal Youtube 'n deel kry van die advertensie-inkomste wat deur jou video's gegenereer word. So 'n ander vraag is, hoekom sal 'n individu of 'n maatskappy wolkrekenaars gebruik? Wel, soos ek net genoem het, is 'n groot rede die koste. Met wolkrekenaars elimineer 'n persoon of maatskappy baie van die koste verbonde aan die aankoop van hul eie hardeware en sagteware, tesame met die gebouonderhoud en elektrisiteit wat dit verg om hul eie datasentrum te bestuur. Dit sal dus meer kostedoeltreffend wees om eerder 'n wolk te gebruik. En nog 'n rede is betroubaarheid. Want wanneer jy 'n wolk huur, is die wolkverskaffer verantwoordelik vir al die datarugsteun en rampherstel. En as een van sy datasentrums afgaan, sal hulle ook verskeie oortollige werwe as 'n rugsteun hê wat sal verseker dat daar geen stilstand is nie. En nog 'n rede is skaalbaarheid. Wolkverskaffers sal 'n 'pay as you go'-metode aanbied waar jy net kan betaal vir wat jy nodig het. So of jy 'n paar rekenaars of baie moet huur, dit maak nie saak nie. So as jy net 'n klein hoeveelheid rekenaars wil huur om te begin, kan jy dit doen. Maar soos jou besigheid uitbrei, het jy die opsie om onmiddellik meer rekenaars te huur om by jou behoeftes te pas En as jy nie soveel rekenaars hoef te huur nie, kan jy dadelik terugskaal om net 'n paar te huur. So wie is die wolkverskaffers vandag? Wel, die belangrikste wolkverskaffers vandag is Amazon Web Services of (AWS). Microsoft Azure, Google Cloud Platform, Alibaba en IBM. Met Amazon Web Services wat die grootste van almal is - wat ongeveer 'n derde van die wolkmarkaandeel neem. Trouens, een van AWS se grootste kliënte is Netflix. Netflix gebruik Amazon Web Services vir byna al sy rekenaar- en bergingsbehoeftes, insluitend databasisse, analise, video-transkodering, ensovoorts. Dus in plaas daarvan om sy eie datasentrum te bou en honderde miljoene dollars te bestee om sy eie data te huisves, het Netflix verkies om dit uit te kontrakteer aan 'n wolkverskaffer wat Amazon is. Dus 'n groot voordeel wat Netflix het om 'n wolk te gebruik, is dat hulle nie hoef te bekommer oor stilstand, sekuriteit, data-rugsteun of die hoë koste om hul eie datasentrum te bou en in stand te hou nie. Hulle kan net Amazon betaal om dit vir hulle te doen. Dit neem dus 'n geweldige las van Netflix af wat hulle in staat stel om te fokus op ander dinge wat met hul besigheid verband hou. Nou is daar drie verskillende tipes wolkrekenaars. Daar is infrastruktuur as 'n diens of (IaaS). Platform as 'n diens of (PaaS) en sagteware as 'n diens of (SaaS). En hierdie drie verskil in beheer en buigsaamheid. Dit is dus aan die gebruiker om te besluit wat by hul behoeftes pas . Die eerste een is dus infrastruktuur as 'n diens. Hierdie tipe is nou basies waar jy die wolkverskaffer 'n gedeelte van jou besigheid gaan laat bestuur wat die hardewaregedeelte gaan wees . Die wolkverskaffer sal die bedieners, berging, virtualisering en die netwerkgedeelte bestuur. Jy aan die ander kant sal steeds beheer oor die sagteware gedeelte hê. Soos die toepassings, data, bedryfstelsel, middelware en looptyd. Enkele voorbeelde van infrastruktuur as 'n diens wat die gewone persoon sal gebruik, is aanlyn datarugsteundienste, soos iDrive en Carbonite wat wolkberging verskaf. En die volgende een word platform as 'n diens genoem. Nou (PaaS) soos (IaaS) laat die wolkverskaffer toe om 'n gedeelte van jou besigheid te bestuur. Maar die wolkverskaffer het meer beheer. In 'n (PaaS) bestuur die wolkverskaffer nie net die hardeware soos bedieners, berging en netwerke nie, maar dit bestuur ook die bedryfstelsel, middelware en looptyd. Jy aan die ander kant is slegs verantwoordelik vir die toepassings en die data. En uiteindelik is daar sagteware as 'n diens of (SaaS) Nou is dit seker verreweg die algemeenste wolkdiens. In hierdie tipe word al die toepassings deur die wolkverskaffer gehuisves. Daar is geen sagteware om op jou rekenaar te installeer nie en geen hardeware om te bestuur nie. Jy kry eenvoudig toegang tot die toepassing en laat dit van jou rekenaar af loop wanneer jy deur die internet aan die wolkdiens koppel. Die wolkverskaffer bestuur dus al die hardeware, sagteware, netwerke, bedryfstelsels en berging. 'n Goeie voorbeeld van (SaaS) is iets wat ek heeltyd gebruik , dit is Google Docs. Google Docs is 'n gratis aanlyn kantoorpakket wat deur 'n webblaaier verkry word. Daar is geen bykomende sagteware wat op jou rekenaar geïnstalleer moet word om Google Docs te gebruik nie. Alles word vanaf jou webblaaier verkry en bestuur. So dit sluit die video oor wolkrekenaars af. Teken asseblief in en dankie dat jy gekyk het."
                }
            ],
            "subskills": [
                "Cloud Service Models: IaaS (Infrastructure as a Service - e.g., AWS EC2, Azure Virtual Machines, Google Compute Engine), PaaS (Platform as a Service - e.g., AWS Elastic Beanstalk, Google App Engine, Heroku), SaaS (Software as a Service - e.g., Salesforce, Google Workspace, Microsoft 365)",
                "Virtualization: Hypervisors (e.g., VMware vSphere, Xen), containerization (Docker, Kubernetes), virtual machines (VMs)",
                "Networking: Virtual Private Clouds (VPCs), subnets, routing tables, load balancing, firewalls (e.g., AWS Security Groups, Azure Network Security Groups)",
                "Data Storage: Object storage (e.g., AWS S3, Azure Blob Storage, Google Cloud Storage), relational databases (e.g., Amazon RDS, Azure SQL Database, Cloud SQL), NoSQL databases (e.g., Amazon DynamoDB, Azure Cosmos DB, Google Cloud Firestore)",
                "Security: Identity and Access Management (IAM), security groups, encryption (at rest and in transit), compliance standards (e.g., ISO 27001, SOC 2)",
                "Serverless Computing: Functions as a Service (FaaS) (e.g., AWS Lambda, Azure Functions, Google Cloud Functions), event-driven architectures",
                "Cloud Migration Strategies: Lift and shift, replatforming, refactoring, repurposing",
                "Cost Optimization: Resource optimization, right-sizing instances, using spot instances, reserved instances"
            ],
            "key_takeaways": [
                "Cloud computing offers scalability, flexibility, and cost-effectiveness compared to on-premise infrastructure.",
                "Understanding different cloud service models is crucial for choosing the right solution for specific needs.",
                "Security is paramount in the cloud; robust security measures must be implemented at all levels.",
                "Cloud adoption requires careful planning, including migration strategies and cost optimization techniques.",
                "Effective cloud management involves monitoring, automation, and continuous improvement.",
                "Utilizing cloud-native services and tools can significantly enhance efficiency and productivity.",
                "Staying updated with the latest cloud technologies and best practices is essential for continuous improvement.",
                "Choosing the right cloud provider depends on factors such as cost, features, and geographic location."
            ],
            "important_info": [
                "Cloud providers often lock in customers through vendor-specific tools and services.",
                "Data security and compliance are critical responsibilities of cloud users.",
                "Understanding service level agreements (SLAs) is crucial for managing expectations and performance.",
                "Cloud computing requires a different skillset compared to traditional IT infrastructure management.",
                "Cloud adoption involves significant changes to organizational processes and culture.",
                "Proper planning and execution are critical for successful cloud migration projects."
            ],
            "summary": "Cloud computing is a transformative technology reshaping how businesses operate and deliver services. Professionals proficient in cloud computing possess a highly valuable skillset applicable across numerous industries.  They understand how to leverage cloud platforms for enhanced scalability, flexibility, and cost-efficiency, implementing secure and reliable infrastructure. This includes designing, deploying, and managing applications and data on cloud platforms, employing appropriate security measures and optimization strategies. The ability to navigate the intricacies of various cloud services and migrate existing systems effectively is paramount.  A solid understanding of cloud security and compliance standards is vital for maintaining data integrity and adhering to industry regulations, ultimately contributing to a company's competitive advantage."
        },
        {
            "skill": "Machine Learning Algorithms",
            "videos": [
                {
                    "video_id": "8xUher8-5_Q",
                    "title": "How I'd Learn ML/AI FAST If I Had to Start Over",
                    "channel": "Tech With Tim",
                    "view_count": 185316,
                    "duration": "10m 43s",
                    "transcript": "AI is changing extremely fast in 2025 and so is the way that you should be learning it. So, in this video, I'm going to break down exactly how I would learn AI and ML if I was starting completely from scratch with all of the knowledge that I have today. Let's get into it. Now, the first thing or step zero on my list would be to make sure that I was thinking like an engineer. Now look, there's a long list of topics that I'm going to share with you here. All things that are important to learn. But none of them matter if you don't build that deep critical thinking skill. The things that separate good software engineers from great software engineers are the ability to break down problems and to think critically. So as you listen to this list, keep in mind that it's not about memorizing concepts. It's about truly understanding what's going on and being able to solve abstract complex problems, which is really where humans come in and where we're not yet being replaced by AI models. Anyways with step zero out of the way, the first thing that I would be focusing on is really diving deep into Python. Now look, obviously there's all kinds of no code tools out there, but if you want to be an effective AI or ML engineer, I do believe that you still do need to know how to code. And the best way to do that is to start with Python. Python is just the easiest language to learn. It's the best for AI and ML. And personally, if I was diving into this, I would be focusing on learning the fundamentals skipping all of the advanced theory, and building automation projects as quickly as possible. That's what Python is really good at, automating tasks, doing things like data science. So, I would start with things like scripting or scraping. So, web scraping for example. Then I would get into things like numpy mapplot, lib, and pandas. and just get really competent working with data sets within Python. I would also focus on learning the basics of APIs. So, how to make a very simple one and how to call APIs from Python. I'd be doing all of this with the goal of building projects as quickly as possible, not getting into the weeds of all of the theoretical concepts and really just getting comfortable writing code in Python so I can use this as a tool later on when I dive more into the advanced AI and ML stuff. So, that's step one. get comfortable with Python and do it in a practical way using a lot of the tools that I just mentioned. That's personally what I would be doing. And by the way guys, what I'm sharing with you here is not necessarily what I would do if I was trying to land a job, but it's purely what I would do to get good at this as quickly as possible. Now, with that in mind, if you are trying to land an AI or ML job, something that you're going to struggle with is finding a program that teaches you practical skills, but actually balances that with real world credibility. Now, that's why I was quite impressed when I came across SimplyLearn, the sponsor of today's video. Now, this is a world's leading online platform for tech and business education, and they've got a full catalog of hands-on boot camps, and their AI and machine learning programs are seriously well put together. These are live instructor-led classes, not just videos, and they're built in collaboration with some of the world's top universities and companies. The curriculum is projectbased careerfocused, and covers tools like Python, TensorFlow, and Chat GPT depending on the path that you choose. Now, they've got thousands of five-star reviews, recommendations from Switch up Course Support, and Forbes, and tons of success stories from students that have completely changed their career after going through the program. Now, if you're serious about getting into AI or ML, then definitely check out Simply Learns Programs. Click the link in the description or the pinned comment to take your first step towards your next big career move. Now, moving on to step number two, and this one I would try to do fairly quickly, and that's to become data literate. What I mean by this is just being familiar working with data. So, I'd want to learn some basic SQL like some joins, some select statements. What actually is SQL? How do you work with this? I would dive much more into something like pandas, learn it with some more advanced operations. And generally, I would just want to be really comfortable working with large sets of data and understanding what that actually means. The reason for that is that in machine learning and AI, pretty much everything comes down to the data. Sure, you can use all of these LLMs, you can use these great tools, but if you don't have good data or you don't know how to work with that, it doesn't matter. You're never going to get a good result. So, I'd want to focus on really becoming data literate at this stage getting good at querying data, managing data, visualizing it, etc. So that in the next steps I already had that core skill built. Now moving on to step number three where the next thing that I would do is start working with AI models immediately. Now in the past I would have recommended learn all of this theory, learn all of these machine learning algorithms before you dive into things like LLMs. However, today it's crazy what you can build with even really limited knowledge. So I'd want to dive into this straight away just to see what's possible and to make sure that I stayed motivated. Now, that means I would start working with things like the OpenAI API immediately, things like the Claude API. I would work with things like Olama for running models locally. I'd start dabbling with things like Langchain and Langraphph and building some basic AI agents on my own computer. I'd learn about vector databases retrieval, augmented generation, and start working with some of those tools and building some relatively simple AI apps using Python and using these different libraries. I'd also work with something like Streamlit, for example for building really simple UIs and data dashboards, and that would teach me quite a bit about what it actually means to build an AI application. I'd get a lot of fundamental coding skills kind of reinforced. And then later, I can go on and learn the more advanced AI and ML stuff, which is what I'm going to move into now. Okay, cool. So, now we're moving on to step four, where I would be taking a step back and learning the core machine learning and AI fundamentals. Now, a lot of people today, they dive straight into LLM, which is what we just did, right? We started working with LLMs, building AI agents, and seeing what's possible with Python and some of those amazing tools. However, once you do that, you definitely should still learn these core algorithms because a lot of times it's really overkill to use an LLM for the type of AI task that you need. So, what I mean by this is I would start focusing on things like regression classification clustering. These are machine learning techniques that have been around for 20, 30, 40 years that still work and that you can still use today. I would start looking at libraries in Python like scikitlearn where I could learn how to implement these machine learning algorithms and I can use them to build some basic ML apps. After that, I would start working with things like neural networks. Again a really popular technique that's been around for a while that a lot of people have seemed to forget about. After null networks, I would look at some basic computer vision stuff and I would start looking at libraries like PyTorch and TensorFlow to build some more complex machine learning applications that don't involve using something like an LLM. Again, the LLM component is super super cool. You should know how to do that. But a lot of the times you simply don't need it and you can build a better application with a lot of these core fundamentals which really aren't that complicated to understand. Okay, so now moving on to the next step which is step number five. After I got the core machine learning techniques and fundamentals out of the way, I would go allin on LLMs and AI agents. Now, look I know this sounds contradictory to what I just said, but once you build this foundation, you now know what's possible without using an LLM. But this is the new buzz. This is what everyone's using. So, you should be super familiar with this as well. And that's why I would dive straight into LLM and AI agents. Now, the first thing I would want to do is actually understand how an LLM works. understand something like GPT generative pre-trained transformers. What does that actually mean? Understand the architecture at a high level. Get into some of the weeds and see what can LLM do, what can they not do, and what are these magical black boxes that everybody's using on the internet. Now after I understood that, I definitely start looking into some no code tools. As much as we can build everything in Python, it's also really useful to use the tools that already exist. As a developer, you can typically use the noode tools better than people that aren't developers. So, I would start looking at tools like Crew AI, Langflow N8N, things like VPY, LiveKit. There's so many different technologies and tools here for building AI agents and utilizing LLMs. And a lot of times you can build something kind of in their UI platform and then you can hook into it from your Python code and make it really customizable. So, that's personally what I would be doing. And anytime I could use a noode tool, I would if it saved me time and it worked for my particular use case. Again, we're talking about practicality here. How do we practically learn this stuff as quick as possible and get stuff done? Well, sometimes that is using tools that already exist. Now similar to this, I would also be learning about things like MCP servers for example. What are those? How do those work? And then I would start looking into a lot of AI code editors as well. This is kind of more of a sidebar. You may have already done this, but I would definitely want to be familiar with tools like Windsurf, Cursor, uh Lovable, Vzero, Bolt, Replet, all of these AI code editors, how they integrate with things like AI agents and how I could use them to be super productive and build some really cool AI apps. It's kind of like the Matrix here. I'm building AI using an AI code editor that's powered by AI, that's powered by an LLM, that's reviewed by AI. So, AI is really everywhere here, but I just wanted to mention those tools because you definitely should be familiar with them. and personally I would want to be learning them and using them a lot. So this leads me to the final and objectively most important step on my list and that would be to build a ton of AI applications. The only way you get good at anything is by doing a lot of it and doing it in a nonstructured way where you're constantly being challenged and you're trying to build something that you have no idea how to build. That's how I got good at programming. Building literally thousands of small programming projects. That's exactly what I would want to be doing here. just trying to solve real world problems using the AI skills that I built. This is going to teach me more than probably anything else that I had on this list. And I'm going to see how to put these skills actually into practice. So, I'd be building apps to automate business workflows to build maybe internal AI assistants or chat bots. Maybe I'd try to build something like a SAS. I don't know. I would just build a ton of different applications here. Anything that actually was real world and applicable to someone just to really harness these skills. So, there you have it, guys. That is what I would do if I was starting over and I wanted to learn AI and ML. Again, this is not what I would do if I wanted to land a job. I would have some different skills on the list. This is purely if I wanted to be competent in this field and be able to build things as quickly as possible. This is what would work for me. I don't know if it would work for you, but I'm curious to hear what you think. So please leave a comment down below. If you enjoyed the video, make sure to leave a like, subscribe to the channel and I will see you in the next one. [Music]"
                },
                {
                    "video_id": "4RixMPF4xis",
                    "title": "AI vs Machine Learning",
                    "channel": "IBM Technology",
                    "view_count": 1436940,
                    "duration": "5m 49s",
                    "transcript": "artificial intelligence and machine learning what's the difference are they the same well some people kind of frame the question this way it's AI versus ml is that the right way to think of this or is it AI equals ml or is it AI is somehow something different than ml so here's three equations I wonder which one is going to be right well let's talk about this first of all when we talk about AI I think it's important to come with definitions because a lot of people have different ideas of what this is so I'm going to assert the simple definition that AI is basically exceeding or matching the capabilities of a human so we're trying to match the intelligence whatever that means and capabilities of a human subject now what could that involve there's a number of different things for instance one of them is the ability to discover to find out new information another is the ability to infer to read in information from other sources that maybe has not been explicitly stated and then also the ability to reason the ability to figure things out I put this and this together and I come up with something else so I'm going to suggest to you this is what AI is and that's the definition we'll use for this discussion now what kinds of things then would be involved if we were talking about doing machine learning well Machine learning I'm going to put that over here is basically a capability we'll start with a Venn diagram machine learning involves predictions or decisions based on data think about this as a very sophisticated form of statistical analysis it's looking for predictions based upon information that we have so the more we feed into the system the more it's able to give us accurate predictions and decisions based upon that data it's something that learns that's the L part rather than having to be programmed when we program a system I have to come up with all the code and if I wanted to do something different I have to go change the code and then get a different outcome in the machine learning situation what I'm doing could be adjusting some models but is different than programming and mostly it's learning the more data that I give to it so it's based on large amounts of information and there's a couple of different fields within couple of different types there is supervised machine learning and as you might guess there's an unsupervised machine learning and the main difference as the name implies is one has more human oversight looking at the training of the data using labels that are superimposed on the data unsupervised is kind of able to run more uh and and find things that were not explicitly stated okay so that's machine learning it turns out that there's a subfield of machine learning that we call Deep learning and what is deep learning well this involves things like neural networks neural networks involve nodes and statistical relationships between those nodes to model the way that our minds work and it's called Deep because we're doing multiple layers of those neural networks now the interesting thing about deep learning is we can end up with some very interesting insights but we might not always be able to tell how the system came up with that it doesn't always show its work fully so we could end up with some really interesting information not know in some cases how reliable that is because we don't know exactly how it was derived but it's still a very important part of all of this realm that we're dealing with so those are two areas and you can see DL is a subset of ml but what about artificial intelligence where does that fit in the Venn diagram and I'm going to suggest to you it is the superset of mldl and a bunch of other things what could the other of things be well we can involve things like natural language processing uh it could be vision so we want a system that's able to see we might even want a system that's able to hear and be able to distinguish what it's hearing and what it's seeing because after all humans are able to do that and that's part of what our brains do is distinguish those kinds of things it can involve other things like the ability to do text to speech so if we take written words Concepts and be able to speak those out so this first one involved being able to see things this is now being able to speak those things as well and then other things that humans are able to do naturally that we often take for granted is motion this is the field of Robotics which is a subset of AI the ability to just do simple things like tie our shoes open and close the door lift something walk somewhere that's all something that would be part of human capabilities and involves certain sorts of perceptions calculations that we do in our brains that we don't even think about so here's what it comes down to it's a Venn diagram and we've got machine learning We've Got Deep learning and we've got AI so I'm going to suggest to you the right way to think about this is not these equations those are not the way to look at it in fact what we should think about this as machine learning is a subset of a high and that's how we need to think about this when I'm doing machine learning in fact I am doing AI when I'm doing these other things I'm doing AI but none of them are all of AI but they're a very important part thanks for watching please remember to like this video And subscribe to this channel so we can continue to bring you content that matters to you"
                }
            ],
            "subskills": [
                "Supervised Learning: Linear Regression, Logistic Regression, Support Vector Machines (SVMs), Decision Trees, Random Forests, Naive Bayes",
                "Unsupervised Learning: K-Means Clustering, Principal Component Analysis (PCA), DBSCAN",
                "Model Evaluation: Precision, Recall, F1-score, AUC-ROC, Confusion Matrix, Cross-validation",
                "Feature Engineering: Data cleaning, transformation, scaling (standardization, normalization), feature selection, dimensionality reduction",
                "Model Selection and Tuning: Hyperparameter optimization (grid search, random search), Regularization (L1, L2), Bias-Variance Tradeoff",
                "Deep Learning: Neural Networks, Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs)",
                "Algorithm Implementation: Scikit-learn, TensorFlow, PyTorch",
                "Data Preprocessing: Handling missing values, outlier detection, data encoding"
            ],
            "key_takeaways": [
                "Understanding the strengths and weaknesses of different algorithms is crucial for selecting the appropriate model for a given problem.",
                "Model evaluation metrics are essential for assessing the performance and reliability of machine learning models.",
                "Effective feature engineering significantly impacts model accuracy and performance.  Poor features lead to poor models, regardless of algorithm sophistication.",
                "Proper data preprocessing is crucial to avoid bias and improve model accuracy.",
                "Bias-variance tradeoff is a key concept to understand for building robust and generalizable models.",
                "Continual learning and adaptation to new advancements in the field are essential for staying current in machine learning."
            ],
            "important_info": [
                "A strong foundation in mathematics (linear algebra, calculus, probability, statistics) is a prerequisite for understanding machine learning algorithms.",
                "Understanding data structures and algorithms is also necessary for efficient implementation and optimization.",
                "Ethical considerations and potential biases in data and algorithms must be carefully addressed.",
                "The choice of algorithm is highly dependent on the nature of the data (size, structure, type) and the problem being solved (classification, regression, clustering).",
                "Real-world data is messy; expect to spend significant time cleaning and preprocessing data."
            ],
            "summary": "Machine learning algorithms are a cornerstone of modern data science and artificial intelligence, enabling businesses to extract valuable insights from data and automate complex tasks.  Professionals skilled in this area can build predictive models for various applications, such as fraud detection, customer segmentation, risk assessment, and medical diagnosis.  A deep understanding of various algorithms, their limitations, and appropriate evaluation metrics is crucial. This skillset is highly sought after across numerous industries, offering significant career advancement opportunities and high earning potential.  Successful professionals demonstrate proficiency in selecting, implementing, and evaluating algorithms effectively, always considering ethical considerations and data quality."
        },
        {
            "skill": "Data Visualization",
            "videos": [
                {
                    "video_id": "dJA7k58zlA8",
                    "title": "How I'd become a data analyst (if i had to start over) in 2025",
                    "channel": "Agatha",
                    "view_count": 882787,
                    "duration": "8m 57s",
                    "transcript": "here's how I'd become a data analyst if I had to start over as a data analyst now with over 10 years experience I have many Lessons Learned and would take a completely different path anyone yes you can be a data analyst all within 6 months of self-study here's how by failing to prepare you are preparing to fail says Benjamin Franklin and without a plan you will get lost in the data analyst Journey so here is a 6mon road map that I personally would use I'd split up the road map into three different parts the skills the projects and the job applications and put a date on the calendar 6 months from now and work backwards from it so in months one to three I will work on skills in month four I would work on projects and then month five and six is the job application process and 6 months is very reasonable if you have about 3 to 4 hours to study every day and everybody has different schedules and different commitments but if that's something that you can commit to this Roat would work for you I also have a video on how I was able to self-study for 4 hours every day and if you're looking for tips there I will link the video above months 1 through three is skills knowing what I know today I would really just focus on learning three tools and that is Excel SQL and Tapo I would learn it in that order and just focus on just those three for 3 months I'm not including python Cloud technology other bi tools because most entry-level data analyst roles will only require you to know those three skills that I listed and from my personal experience as well those were the only three tools I used for years so that is enough to get you started and you can worry about learning the other skills after you Le the job Excel is the first tool that I would learn because Excel is still the most popular bi tool used across Industries and it's highly likely that in your first role as a data analyst you'll be using a lot of excel you need to know how to clean analyze and present data using Excel and what level do you need about an intermediate level where you know key functions like vup pivot tables how to create visualizations and what I would do in my road map is break it down into smaller pieces so spend a week learning just the formulas and getting used to them and then spend the following week on visualizations and thinking through how you can visualize data using charts and graphs the best way to learn is by doing rather than moving on to SQL as soon as you finish Excel take the time to practice it what I would do is take my bank statement and try to do some analysis you can try answering questions like what does the trend of my spending look like and this would require taking all your spending and putting it on a line graph where you can see the trend of the increases and decreases of spending over time and then you can try bucketing your spending into categories like entertainment food and chart those categories over time to see how you're spending changes by category over time and this is the type of analysis that a data analyst would do next is seel seel is fundamental to be a data analyst if you have a technical assessment for your data analyst job it will 100% be in SQL data is stored in databases and SQL is a language that you use to talk to databases to get the information that you need SQL is a more powerful tool than Excel because it allows you to work with really large data set and also be able to combine them to easily create different analysis you only need to be an intermediate level SQL user note I didn't say Advanced and that includes functions like select wear Group by having joins and window functions some websites I personally use to learn SQL are data Camp Udi LinkedIn learning data Camp was extremely useful because it allowed me to interactively learn SQL handson without having to download anything as well as LinkedIn learning if you are in the US a lot of public libraries actually offer a free subscription so I was able to do a lot of my learning for free through Linkedin learning and lastly don't forget YouTube because there's a lot of really good free tutorials on YouTube that you should use as well next we move on to Tableau now any bi visualization tool can be used in place of Tableau such as powerbi or looker or quick ey but I personally would learn Tableau because it is the most commonly used tool that you'll see on job listings also from my experience when you start learning just one bi tool you know about 80% of others so it's really not as important which one you pick here so what should you know in Tau you should be about an intermediate user that knows how to connect to data add multiple data sources as well as creating visualizations with filters will get you pretty far there's also a free version of Tableau that you can download and start practicing and you can even take the same bank statement data that you had earlier and use that to create some of the same visualizations to get practice month four is projects this is a step that I wouldn't skip because without any data work experience I don't have anything to show recruiters that I know how to do data analysis so knowing that the purpose of these project projects is to use on my resume and interviews I would strategically use them to make myself stand out here are three tips for your projects tip one I'd create three to four projects that shows a combination of my skills so I wouldn't create a project in just Excel and a project in just SQL I would do a combination of Excel and SQL or SQL in Tableau and this shows the recruiter that I know which tools to use to solve which problems tip number two I create projects that have an analysis that solves a problem and tells a story data analysts always start their process first with the problem and it's no different for a project a common mistake is to start with the data set first with no plan and then work aimlessly without knowing what it is that you're going to do it's like when I'm looking at the bank statement data and without a problem statement I would just take it and create all sorts of fancy graphs that look really good but don't mean anything it's only when I have the problem statement of what does a trend of my spending look like that I actually have a direction and can sit down and think about well how would I best show this through a visualization and tell a story tip number three use free resources for data such as kaggle Reddit data.gov and even corser does have guided projects that you can pay for months 5 through 6 is the job application process I would prepare for interviews by updating my resume my LinkedIn and applying for data analyst jobs and I do have a video on how to create a data analyst resume with actionable tips which I'll leave up there for you to watch while applying for jobs I would start practicing technical questions pretty much every single data analyst job that I applied to has required a SQL technical assessment along with the general interview and I didn't know this and was completely unprepared but you know better so in month 5 give yourself enough time to take some technical test so that when it comes time for the actual interview you'll be ready I've personally used Le code hacker rank for SQL interview questions and they were actually pretty accurate to the ones that I gotten in real life so try and do the medium easy questions and if you can pass those then you'll be prepared with this R map I would start and then learn not learn and then start I hope this is the video that starts you on your data analyst journey and if you're interested in learning more about the soft skills a data analyst uses watch how to get ahead of 99% of data analyst and I will see you there"
                },
                {
                    "video_id": "SqjGq275d0M",
                    "title": "Introducing BigQuery data engineering agents",
                    "channel": "Google Cloud Tech",
                    "view_count": 10721,
                    "duration": "6m 20s",
                    "transcript": "This AI powered agent can create SQL code, metadata, pipelines, and more with just prompts. The data engineering agent in BigQuery is here to help data analysts and data engineers do more in less time. This agent can take your prompts and help you become more productive by creating and modifying data pipelines, generating and modifying schemas, helping with troubleshooting, and assisting with metadata required for your own AI powered applications. For context, I have replicated tables from Salesforce APIs containing information about support cases. My goal is to make it easier for my business users to consume the data and gain insights from it. I also want to make sure users have metadata available for AI agents. Let's see if I can show you the first iteration of an initial load pipeline in less than 10 minutes. So, we start by creating a pipeline and setting it to the US location, which is where all of my data is. I will start by creating the data set first with this very simple prompt. Create a data set called SFDC data set in location US. We can see the agent is creating the definitions for the data set and compiling the pipeline also checking for errors. Once it finishes, we get the SQL X definition to create the data set. Now I accept the changes suggested by the agent. Then I run the pipeline. So the creation step executes and we can see this step is fast and successful. And just like that, I have a data set. So we can now move on to something more exciting. For this example, I landed the data into a storage bucket just to show you how this initial load works. So let's ask the agent to load all of my files from the GCS bucket. I'm asking it to create physical tables based on the schemas in the parquet files to use the same names in the files for the tables. And I'm explicitly listing the files I wanted to use as a better practice for prompting. This will take a few minutes, but we can see it scanned the data file by file. It's creating the SQL X definitions for each table and then is validating them. One thing I really like about this agent is that it gives an explanation of what it's doing and why after it finishes with each prompt. If we look at one of the tables it will generate, we have all of the fields from the parquet files followed by the data load operation that I requested in my prompt. Let's apply the suggestions and run our pipeline. And just like that, we have tables with data. Now, you may have noticed different timestamp fields in my tables and these record different events related to support cases. I know that my users will want to report on this by quarter. So, let's add a time dimension for them. Let's take a look at the definition it's created. And it looks solid. But before we run the changes and take a look, let's also enhance the cases table so that we can easily filter out those cases with profanities. Unfortunately, sometimes our customers are very frustrated and our cases table has these words in one of the text fields. We can use one of BigQuery's AI functions with a prompt to look for inappropriate terms. We can do this thanks to a connection I created to Vertex AI that's serving a Gemini model. We can see it has created a table that will be filled with all the existing fields plus a boolean field to filter out cases with profanities. So let's recap while this operation runs for a couple of minutes. So far we have created a data set, loaded the tables from the storage bucket, enriched the cases schema including an AI function calling Gemini served in Vert.ex AI and generated a time dimension. All of this with prompts. One aspect we always want to take care of is data quality. So let's see what our agent can do to help us. And we'll choose the user table first. We can see it detected incorrect changes it tried to make during one of the validations. So, it reverted them autonomously. This is super smart. For data quality, it's come up with checking for some fields not being null and for the email address to contain the right format. Let's check one of the definitions that will be later converted into assertion tests in SQL. Finally, we said we wanted to make this data available to AI agents for things like conversational analytics. So, let's ask the agent to create metadata for our tables. And just as expected, we can see the metadata it has generated. As a side note, it's always a good idea to enrich this with your own business terminology and synonyms, but this has saved us a lot of time of dull and manual work. Let's run this pipeline and check the results. While this is running, we can already see the metadata applied to the existing tables. And if we go back to the pipeline, we can also see one of the data quality validations has failed, which is actually a successful failure because I know that my data has some issues. And as the rest of the pipeline is completing successfully, we can look into the generated tables and views. We can see our brand new data set, the original tables loaded from a GCS bucket, a table using an advanced AI function to detect profanity, the time dimension we added through the agent, the metadata generated for each of the fields, and the data quality assertions. The agent generated this initial load pipeline and the data is now ready for the users to get insights and make informed decisions. If you want to give this a try, check the link in the comments and let the data engineering agent in BigQuery make your job easier. [Music] [Music]"
                }
            ],
            "subskills": [
                "Data Wrangling & Cleaning: Handling missing values, outlier detection, data transformation, data normalization.",
                "Choosing the Right Chart Type: Bar charts, line graphs, scatter plots, pie charts, heatmaps, histograms, appropriate use for different data types.",
                "Data Exploration & Analysis: Identifying patterns, trends, and anomalies within datasets.",
                "Color Theory & Aesthetics: Effective use of color palettes, font choices, and visual hierarchy to improve clarity and readability.",
                "Interactive Visualization: Creating dynamic and interactive visualizations using tools like Tableau, Power BI, or D3.js.",
                "Storytelling with Data: Communicating insights effectively using visuals to support narratives and conclusions.",
                "Statistical Graphics: Using statistical methods (e.g., regression lines, error bars) to enhance visualizations.",
                "Geospatial Visualization: Representing data geographically using maps and other location-based visualizations."
            ],
            "key_takeaways": [
                "Effective data visualization should focus on communicating key insights clearly and concisely, rather than displaying all data.",
                "The choice of visualization should be driven by the type of data and the story to be told.",
                "Clear labeling, titles, and legends are essential for understanding the visualization.",
                "Data visualizations should be accessible and understandable to a broad audience, regardless of their technical expertise.",
                "Iteration and refinement are crucial; visualizations often require multiple revisions before they effectively communicate insights.",
                "Context is key: visualizations should always be accompanied by relevant background information and interpretations."
            ],
            "important_info": [
                "Proficiency in data manipulation and analysis tools is necessary (e.g., SQL, Python with Pandas/NumPy).",
                "An understanding of statistical concepts is beneficial for insightful visualizations.",
                "Accessibility considerations are paramount: visualizations must be easily understood by everyone.",
                "Misleading visualizations can cause significant harm; responsible data visualization requires ethical awareness.",
                "Industry-standard tools and software vary; familiarity with common programs is often a job requirement."
            ],
            "summary": "Data visualization is a critical skill for effectively communicating complex data insights across all industries. Professionals proficient in data visualization can transform raw data into easily understood narratives, revealing trends, patterns, and anomalies. This skill is highly valued across numerous roles, from data analysts and scientists to business intelligence professionals and marketing specialists. Mastering data visualization enhances decision-making, improves communication, and allows for more impactful data-driven strategies.  The ability to create compelling, informative, and ethical visualizations is a highly sought-after competency in today's data-centric world, enhancing career prospects and fostering greater professional impact."
        }
    ],
    "important_considerations": [
        "**Continuous Learning:** The AI/Data Engineering field is constantly evolving. Stay updated with new technologies, algorithms, and best practices through online courses, conferences, and industry publications.",
        "**Cloud Provider Specialization:** While knowing multiple cloud platforms is beneficial, focusing on one initially helps you gain in-depth expertise and certifications, making you a more marketable candidate.",
        "**Ethical Considerations:**  Understand the ethical implications of AI and data usage, including bias in algorithms and data privacy.  This is increasingly important for employers.",
        "**Networking and Collaboration:** Build your network through online communities, conferences, and meetups. Collaborating on projects demonstrates teamwork and expands your knowledge.",
        "**Portfolio Development:** A strong portfolio demonstrating practical skills is crucial.  Contribute to open-source projects or create personal projects showcasing your abilities.",
        "**Certifications:** Relevant certifications (AWS, Azure, GCP, etc.) can significantly boost your resume and demonstrate expertise to potential employers.",
        "**Domain Knowledge:**  Gaining expertise in a specific industry (finance, healthcare, etc.) can make you a highly sought-after specialist."
    ],
    "learning_path": [
        "**Step 1: Foundational Programming and Statistics:**  Master Python (including libraries like NumPy, Pandas) and fundamental statistical concepts (descriptive statistics, probability distributions, hypothesis testing).  Complete online courses or take university-level introductory courses.",
        "**Step 2: SQL and Database Management:** Learn SQL comprehensively, focusing on querying, data manipulation, and database design. Gain practical experience with relational databases (e.g., PostgreSQL, MySQL) and explore NoSQL databases (MongoDB, Cassandra).  Consider online courses and hands-on projects using publicly available datasets.",
        "**Step 3: Data Mining Fundamentals:** Focus on data cleaning, preprocessing, exploration, and visualization using Python libraries like Matplotlib and Seaborn.  Learn and apply basic data mining algorithms (linear regression, logistic regression, k-means clustering).  Work through structured projects to build your portfolio.",
        "**Step 4: Big Data Technologies Introduction:** Learn Hadoop and Spark fundamentals, understanding distributed computing principles and working with large datasets.  Explore cloud storage options (AWS S3, Azure Blob Storage, GCP Cloud Storage) and ETL processes.  Participate in Kaggle competitions or undertake personal projects involving large datasets.",
        "**Step 5: Advanced Machine Learning and Model Deployment:** Deepen your understanding of machine learning algorithms (including advanced techniques like Random Forests, SVMs, and deep learning basics). Learn model evaluation metrics, hyperparameter tuning, and cross-validation techniques. Explore cloud-based machine learning platforms (AWS SageMaker, Azure Machine Learning, Google Vertex AI) and deploy your models.",
        "**Step 6: Cloud Computing Specialization:** Choose a cloud provider (AWS, Azure, or GCP) and gain expertise in its services relevant to data engineering. This includes IaaS, PaaS, data storage, networking, and security. Obtain relevant certifications (AWS Certified Data Analytics – Specialty, Azure Data Engineer Associate, Google Cloud Professional Data Engineer).",
        "**Step 7: Data Visualization and Communication:**  Master data visualization tools like Tableau or Power BI. Learn to create compelling and insightful visualizations to effectively communicate data findings to both technical and non-technical audiences. Practice presenting your work and building effective data storytelling skills.",
        "**Step 8:  Portfolio Building and Networking:** Create a strong portfolio showcasing your projects and skills.  Actively network within the AI and data science community, attending conferences, meetups, and engaging online.  Seek mentorship and build relationships with experienced professionals."
    ],
    "created_at": "2025-09-08T12:46:18.031438",
    "role_summary": "The AI Data Engineer plays a critical role in driving business value through the development and implementation of robust, scalable data solutions for machine learning initiatives.  This involves mining, processing, and transforming large datasets using Big Data technologies and cloud computing platforms.  Excellence in this role is demonstrated by the design and implementation of efficient data pipelines that feed high-quality data to machine learning algorithms, resulting in improved model accuracy and performance.  A deep understanding of machine learning algorithms and strong data visualization skills are essential for communicating insights and driving data-informed decision-making, ultimately contributing to enhanced business outcomes."
}